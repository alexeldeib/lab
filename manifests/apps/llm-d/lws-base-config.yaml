# A universal baseconfig for models that can be downloaded from Hugging Face
apiVersion: v1
kind: ConfigMap
metadata:
  name: vllm-init-scripts-config
  namespace: llm-d
data:
  init-vllm.sh: |
    #!/usr/bin/env bash
    ##############################################################################
    # vLLM bootstrap script
    # - Installs vLLM
    # - Idempotent: re-runs will update existing repos instead of recloning
    ##############################################################################

    set -euo pipefail
    trap 'echo "ERROR: Script failed on line $LINENO"; exit 1' ERR

    ###############################  helpers  ####################################
    banner() { printf '\n========== %s ==========\n' "$*"; }

    # Re-usable “uv pip install” wrapper (adds --no-cache-dir by default)
    upip() { "${UV}" pip install --python "${PYTHON}" --no-progress --no-color --no-cache-dir "$@"; }

    # Clone the repo if missing, otherwise fast-forward to the requested branch
    clone_or_update() {
      local url=$1 dir=$2 branch=${3:-main}
      if [[ -d "${dir}/.git" ]]; then
        banner "Updating $(basename "${dir}")"
        git -C "${dir}" fetch --depth=1 origin "${branch}"
        git -C "${dir}" checkout "${branch}"
        git -C "${dir}" reset --hard "origin/${branch}"
      else
        banner "Cloning $(basename "${dir}")"
        git clone --depth=1 --branch "${branch}" "${url}" "${dir}"
      fi
    }

    ##############################  configuration  ###############################
    # Locations (override via env if desired)
    VLLM_SOURCE_DIR="${VLLM_SOURCE_DIR:-/app/vllm}"
    VENV_PATH="/app/venv"

    # Python / toolchain
    PYTHON_VERSION="${PYTHON_VERSION:-3.12}"          # e.g. 3.12
    PYTHON_COMMAND="${PYTHON_COMMAND:-python${PYTHON_VERSION}}"
    PY_TAG="${PYTHON_VERSION//./}"                   # 3.12 → 312 for wheel tag
    UV="${UV_INSTALL_PATH:-/usr/local/bin/uv}"

    # Repositories
    VLLM_REPO_URL="${VLLM_REPO_URL:-https://github.com/vllm-project/vllm.git}"
    VLLM_BRANCH="${VLLM_BRANCH:-main}"
    DOTFILES_REPO_URL="${DOTFILES_REPO_URL:-https://github.com/tlrmchlsmth/dotfiles.git}"

    # Build-time env
    export TORCH_CUDA_ARCH_LIST="9.0a+PTX"

    #############################  sanity checks  ################################
    command -v git   >/dev/null || { echo "git not found";   exit 1; }
    command -v "${UV}" >/dev/null || { echo "uv not found at ${UV}"; exit 1; }

    banner "Environment summary"
    echo "Python version      : ${PYTHON_VERSION}"
    echo "Virtualenv path     : ${VENV_PATH}"
    echo "uv binary           : ${UV}"
    echo "vLLM repo / branch  : ${VLLM_REPO_URL}  (${VLLM_BRANCH})"
    echo "====================================================================="

    PYTHON="${VENV_PATH}/bin/python"

    # --------------------------------------------------------------------------
    # Ensure the venv has pip so 'python -m pip' works later (DeepEP build step)
    # --------------------------------------------------------------------------
    banner "Bootstrapping pip inside venv"
    "${PYTHON}" -m ensurepip --upgrade
    "${PYTHON}" -m pip install -U pip wheel setuptools

    ################################  vLLM  ######################################
    clone_or_update "${VLLM_REPO_URL}" "${VLLM_SOURCE_DIR}" "${VLLM_BRANCH}"

    banner "Installing vLLM (editable)"
    pushd "${VLLM_SOURCE_DIR}" >/dev/null
    upip -e .
    popd >/dev/null

    ################################  Dotfiles  ######################################
    banner "Installing dotfiles"
    clone_or_update "${DOTFILES_REPO_URL}" "${HOME}/dotfiles"
    pushd "${HOME}/dotfiles" >/dev/null

    # Run dotfiles installer 
    # Don't log in with GH_TOKEN_FROM_SECRET, because it takes too long.
    bash ./install.sh

    popd >/dev/null

    ##############################################################################
    banner "All components installed successfully – vLLM is ready!"
    ##############################################################################

---

# Requirements:
# Any consuming ModelService should define ports labeled:
#    - app_port - the external port number for the prefill and decode pods
#    - internal_port - the port number used by the sidecar to communicate with a vllm container
apiVersion: v1
kind: ConfigMap
metadata:
  name: lws-baseconfig
  namespace: llm-d
data:
  decodeLeaderWorkerSet: |
    apiVersion: leaderworkerset.x-k8s.io/v1
    kind: LeaderWorkerSet
    metadata:
        name: vllm-decode
    spec:
        startupPolicy: LeaderCreated
        leaderWorkerTemplate:
            size: {{ div .DecodeTensorParallelism 8 }}
            restartPolicy: RecreateGroupOnPodRestart

            leaderTemplate:
                spec:
                  initContainers:
                  - name: routing-proxy
                    args:
                    - "--port={{ "app_port" | getPort }}"
                    - "--vllm-port={{ "internal_port" | getPort }}"
                    - --connector=nixlv2
                    - -v=6
                    image: ghcr.io/llm-d/llm-d-routing-sidecar:0.0.6
                    imagePullPolicy: Always
                    ports:
                    - containerPort: {{ "app_port" | getPort }}
                      protocol: TCP
                    resources: {}
                    restartPolicy: Always
                    securityContext:
                      allowPrivilegeEscalation: false
                      runAsNonRoot: true
                  containers:
                  - name: vllm-worker
                    # image: docker.io/alexeldeib/vllm:25e7af5@sha256:f710d0ce52daba6e35bb2078f2ca1318d5c0c3b4e3b62c6677d063c12d568bee
                    # image: docker.io/alexeldeib/vllm:25e7af5-docker-pypi@sha256:ddf630df17d3cb40ebea622f418eafd4eef048761268b2fdfd3b8b873a3486e3
                    # image: docker.io/alexeldeib/vllm:25e7af5-depot-src@sha256:2ff1117900b55a6c62364b84489209ca2ade59f137a045d383e9d3f7f3b4b287
                    image: docker.io/alexeldeib/llm-d:0.0.8-25e7af5
                    # image: "docker.io/alexeldeib/vllm:25e7af5@sha256:333996559c076c87e7e300f99807bf37585aa1e51487075fac35714853ffe834"
                    imagePullPolicy: Always
                    workingDir: /app
                    stdin: true
                    tty: true
                    command: ["/bin/sh","-c"]
                    args:
                      - |
                        # Squash a warning.
                        rm /etc/libibverbs.d/vmw_pvrdma.driver
                        #################
                        # Install vLLM
                        #################
                        ### /init-scripts/init-vllm.sh
                        #################
                        # RUN vLLM
                        #################
                        START_RANK=$(( ${LWS_WORKER_INDEX:-0} * DP_SIZE_LOCAL ))
                        if [ "${LWS_WORKER_INDEX:-0}" -eq 0 ]; then
                          #################
                          # Leader-only launch
                          #################
                          ray start --head --block &
                          vllm serve \
                            {{ .HFModelName }} \
                            --served-model-name {{ .HFModelName }} \
                            --port {{ "internal_port" | getPort }} \
                            --disable-log-requests \
                            --enable-expert-parallel \
                            --tensor-parallel-size $TP_SIZE \
                            --data-parallel-size $DP_SIZE \
                            --data-parallel-size-local $DP_SIZE_LOCAL \
                            --data-parallel-address $(LWS_LEADER_ADDRESS).svc.cluster.local \
                            --data-parallel-rpc-port 5555 \
                            --trust-remote-code \
                            --enforce-eager
                        else
                          #################
                          # Worker-only launch
                          #################
                          ray start --block --address=$(LWS_LEADER_ADDRESS).svc.cluster.local:6379
                          # exec vllm serve \
                          #   {{ .HFModelName }} \
                          #   --served-model-name {{ .HFModelName }} \
                          #   --port {{ "internal_port" | getPort }} \
                          #   --disable-log-requests \
                          #   --enable-expert-parallel \
                          #   --tensor-parallel-size $TP_SIZE \
                          #   --data-parallel-size $DP_SIZE \
                          #   --data-parallel-size-local $DP_SIZE_LOCAL \
                          #   --data-parallel-address $(LWS_LEADER_ADDRESS).svc.cluster.local \
                          #   --data-parallel-rpc-port 5555 \
                          #   --trust-remote-code \
                          #   --kv-transfer-config \
                          #     '{"kv_connector":"NixlConnector","kv_role":"kv_both"}' \
                          #   --enforce-eager
                        fi
                    env:
                      - name: RAY_CGRAPH_get_timeout
                        value: "600"
                      - name: DP_SIZE
                        value: "{{ .DecodeDataParallelism }}"
                      - name: TP_SIZE
                        value: "{{ .DecodeTensorParallelism }}"
                      - name: DP_SIZE_LOCAL
                        value: "1"
                      - name: VLLM_REPO_URL
                        value: "https://github.com/vllm-project/vllm.git"
                      - name: VLLM_BRANCH
                        value: "main"
                      - name: VLLM_ALL2ALL_BACKEND
    #                    value: "naive"
                        # value: "pplx"
                        value: "deepep_high_throughput"
    #                    value: "deepep_low_latency"
    #
                        # Needed for GDRCOPY to be used.
                        # See: https://github.com/NVIDIA/nvidia-container-toolkit/releases/tag/v1.15.0
                      - name: NVIDIA_GDRCOPY
                        value: "enabled"
    #                  - name: NVIDIA_NVSWITCH
    #                    value: "enabled"
    #                  - name: NVIDIA_GDS
    #                    value: "enabled"

                      # NVIDIA_MOFED is likely needed for using IBGDA but causes crashes
    #                  - name: NVIDIA_MOFED
    #                    value: "enabled"
    #
                      - name: NCCL_DEBUG
                        value: "INFO"
                      - name: NVSHMEM_DEBUG
                        value: "TRACE"
                      - name: NVSHMEM_DEBUG_SUBSYS
                        value: "TRANSPORT,INIT,MEM,COLL,BOOTSTRAP"
                      - name: NVSHMEM_REMOTE_TRANSPORT
                        value: "ibrc"
                      - name: NVSHMEM_IB_ENABLE_IBGDA
                        value: "true"
                      - name: NVSHMEM_ENABLE_NIC_PE_MAPPING
                        value: "true"
                      - name: NVSHMEM_HCA_LIST
                        value: "ibp0:1,ibp1:1,ibp2:1,ibp3:1,ibp4:1,ibp5:1,ibp6:1,ibp7:1"
                      - name: NVSHMEM_BOOTSTRAP_UID_SOCK_IFNAME
                        value: "eth0"
                      - name: GLOO_SOCKET_IFNAME
                        value: "eth0"
                      - name: NCCL_SOCKET_IFNAME
                        value: "eth0"
                      - name: NCCL_IB_HCA
                        value: "ibp"
                      - name: UCX_NET_DEVICES
                        value: "ibp0:1,ibp1:1,ibp2:1,ibp3:1,ibp4:1,ibp5:1,ibp6:1,ibp7:1"
                      - name: VLLM_LOGGING_LEVEL
                        value: "DEBUG"
                      - name: HF_TOKEN
                        valueFrom:
                          secretKeyRef:
                            name: llm-d-hf-token
                            key: HF_TOKEN
                            optional: true
                      - name: GH_TOKEN_FROM_SECRET
                        valueFrom:
                          secretKeyRef:
                            name: gh-token-secret
                            key: GH_TOKEN
                            optional: true
                      - name: LMCACHE_DISTRIBUTED_URL
                        valueFrom:
                          fieldRef:
                            fieldPath: status.podIP
                      - name: UCX_TLS
                        value: "^cuda_ipc"
                      # - name: LMCACHE_ENABLE_DEBUG
                      #   value: "True"
                      # - name: LMCACHE_LOCAL_CPU
                      #   value: "True"
                      # - name: LMCACHE_MAX_LOCAL_CPU_SIZE
                      #   value: "5"
                      # - name: LMCACHE_MAX_LOCAL_DISK_SIZE
                      #   value: "10"
                      # - name: LMCACHE_CHUNK_SIZE
                      #   value: "256"
                      # - name: LMCACHE_LOOKUP_URL
                      #   value: llm-d-redis-master.llm-d.svc.cluster.local:8100
                      - name: VLLM_NIXL_SIDE_CHANNEL_PORT
                        value: "6555"
                      - name: VLLM_NIXL_SIDE_CHANNEL_HOST
                        valueFrom:
                          fieldRef:
                            fieldPath: status.podIP

                    securityContext:
                      capabilities:
                        add: [ "IPC_LOCK" ]
                    # startupProbe:
                    #   httpGet:
                    #     path: /health
                    #     port: {{ "app_port" | getPort }}
                    #   failureThreshold: 60
                    #   initialDelaySeconds: 15
                    #   periodSeconds: 30
                    #   timeoutSeconds: 5
                    # livenessProbe:
                    #   tcpSocket:
                    #     port: {{ "app_port" | getPort }}
                    #   failureThreshold: 3
                    #   periodSeconds: 5
                    # readinessProbe:
                    #   httpGet:
                    #     path: /health
                    #     port: {{ "app_port" | getPort }}
                    #   failureThreshold: 3
                    #   periodSeconds: 5
                    resources:
                      limits:
                        nvidia.com/gpu: 8
                        ephemeral-storage: 256Gi
                        rdma/ib: 1
                      requests:
                        cpu: 8
                        memory: 750Gi
                        ephemeral-storage: 256Gi
                        nvidia.com/gpu: 8
                        rdma/ib: 1
                    volumeMounts:
                      - mountPath: /dev/shm
                        name: dshm
                      - name: init-scripts-volume
                        mountPath: /init-scripts
                  volumes:
                    # Volume for the init script from ConfigMap
                    - name: init-scripts-volume
                      configMap:
                        name: vllm-init-scripts-config
                        defaultMode: 0755 # Set execute permissions for the script
                    # Needed for NCCL to function
                    - name: dshm
                      emptyDir:
                        medium: Memory
                        sizeLimit: 16Gi

            workerTemplate:    
                spec:
                  containers:
                  - name: vllm-worker
                    # image: docker.io/alexeldeib/vllm:25e7af5@sha256:f710d0ce52daba6e35bb2078f2ca1318d5c0c3b4e3b62c6677d063c12d568bee
                    # image: docker.io/alexeldeib/vllm:25e7af5-docker-pypi@sha256:ddf630df17d3cb40ebea622f418eafd4eef048761268b2fdfd3b8b873a3486e3
                    # image: docker.io/alexeldeib/vllm:25e7af5-depot-src@sha256:2ff1117900b55a6c62364b84489209ca2ade59f137a045d383e9d3f7f3b4b287
                    image: docker.io/alexeldeib/llm-d:0.0.8-25e7af5
                    # image: "docker.io/alexeldeib/vllm:25e7af5@sha256:333996559c076c87e7e300f99807bf37585aa1e51487075fac35714853ffe834"
                    imagePullPolicy: Always
                    workingDir: /app
                    stdin: true
                    tty: true
                    command: ["/bin/sh","-c"]
                    args:
                      - |
                        # Squash a warning.
                        rm /etc/libibverbs.d/vmw_pvrdma.driver
                        #################
                        # Install vLLM
                        #################
                        ### /init-scripts/init-vllm.sh
                        #################
                        # RUN vLLM
                        #################
                        START_RANK=$(( ${LWS_WORKER_INDEX:-0} * DP_SIZE_LOCAL ))
                        if [ "${LWS_WORKER_INDEX:-0}" -eq 0 ]; then
                          #################
                          # Leader-only launch
                          #################
                          ray start --head
                          exec vllm serve \
                            {{ .HFModelName }} \
                            --served-model-name {{ .HFModelName }} \
                            --port {{ "internal_port" | getPort }} \
                            --disable-log-requests \
                            --enable-expert-parallel \
                            --tensor-parallel-size $TP_SIZE \
                            --data-parallel-size $DP_SIZE \
                            --data-parallel-size-local $DP_SIZE_LOCAL \
                            --data-parallel-address $(LWS_LEADER_ADDRESS).svc.cluster.local \
                            --data-parallel-rpc-port 5555 \
                            --trust-remote-code \
                            --enforce-eager
                        else
                          #################
                          # Worker-only launch
                          #################
                          ray start --block --address=$(LWS_LEADER_ADDRESS).svc.cluster.local:6379
                          # exec vllm serve \
                          #   {{ .HFModelName }} \
                          #   --served-model-name {{ .HFModelName }} \
                          #   --port {{ "internal_port" | getPort }} \
                          #   --disable-log-requests \
                          #   --enable-expert-parallel \
                          #   --tensor-parallel-size $TP_SIZE \
                          #   --data-parallel-size $DP_SIZE \
                          #   --data-parallel-size-local $DP_SIZE_LOCAL \
                          #   --data-parallel-address $(LWS_LEADER_ADDRESS).svc.cluster.local \
                          #   --data-parallel-rpc-port 5555 \
                          #   --trust-remote-code \
                          #   --kv-transfer-config \
                          #     '{"kv_connector":"NixlConnector","kv_role":"kv_both"}' \
                          #   --enforce-eager
                        fi
                    env:
                      - name: RAY_CGRAPH_get_timeout
                        value: "600"
                      - name: DP_SIZE
                        value: "{{ .DecodeDataParallelism }}"
                      - name: TP_SIZE
                        value: "{{ .DecodeTensorParallelism }}"
                      - name: DP_SIZE_LOCAL
                        value: "1"
                      - name: VLLM_REPO_URL
                        value: "https://github.com/vllm-project/vllm.git"
                      - name: VLLM_BRANCH
                        value: "main"
                      - name: VLLM_ALL2ALL_BACKEND
    #                    value: "naive"
                        # value: "pplx"
                        value: "deepep_high_throughput"
    #                    value: "deepep_low_latency"
    #
                        # Needed for GDRCOPY to be used.
                        # See: https://github.com/NVIDIA/nvidia-container-toolkit/releases/tag/v1.15.0
                      - name: NVIDIA_GDRCOPY
                        value: "enabled"
    #                  - name: NVIDIA_NVSWITCH
    #                    value: "enabled"
    #                  - name: NVIDIA_GDS
    #                    value: "enabled"

                      # NVIDIA_MOFED is likely needed for using IBGDA but causes crashes
    #                  - name: NVIDIA_MOFED
    #                    value: "enabled"
    #
                      - name: NCCL_DEBUG
                        value: "INFO"
                      - name: NVSHMEM_DEBUG
                        value: "TRACE"
                      - name: NVSHMEM_DEBUG_SUBSYS
                        value: "TRANSPORT,INIT,MEM,COLL,BOOTSTRAP"
                      - name: NVSHMEM_REMOTE_TRANSPORT
                        value: "ibrc"
                      - name: NVSHMEM_IB_ENABLE_IBGDA
                        value: "true"
                      - name: NVSHMEM_ENABLE_NIC_PE_MAPPING
                        value: "true"
                      - name: NVSHMEM_HCA_LIST
                        value: "ibp0:1,ibp1:1,ibp2:1,ibp3:1,ibp4:1,ibp5:1,ibp6:1,ibp7:1"
                      - name: NVSHMEM_BOOTSTRAP_UID_SOCK_IFNAME
                        value: "eth0"
                      - name: GLOO_SOCKET_IFNAME
                        value: "eth0"
                      - name: NCCL_SOCKET_IFNAME
                        value: "eth0"
                      - name: NCCL_IB_HCA
                        value: "ibp"
                      - name: UCX_NET_DEVICES
                        value: "ibp0:1,ibp1:1,ibp2:1,ibp3:1,ibp4:1,ibp5:1,ibp6:1,ibp7:1"
                      - name: VLLM_LOGGING_LEVEL
                        value: "DEBUG"
                      - name: HF_TOKEN
                        valueFrom:
                          secretKeyRef:
                            name: llm-d-hf-token
                            key: HF_TOKEN
                            optional: true
                      - name: GH_TOKEN_FROM_SECRET
                        valueFrom:
                          secretKeyRef:
                            name: gh-token-secret
                            key: GH_TOKEN
                            optional: true
                      - name: LMCACHE_DISTRIBUTED_URL
                        valueFrom:
                          fieldRef:
                            fieldPath: status.podIP
                      - name: UCX_TLS
                        value: "^cuda_ipc"
                      # - name: LMCACHE_ENABLE_DEBUG
                      #   value: "True"
                      # - name: LMCACHE_LOCAL_CPU
                      #   value: "True"
                      # - name: LMCACHE_MAX_LOCAL_CPU_SIZE
                      #   value: "5"
                      # - name: LMCACHE_MAX_LOCAL_DISK_SIZE
                      #   value: "10"
                      # - name: LMCACHE_CHUNK_SIZE
                      #   value: "256"
                      # - name: LMCACHE_LOOKUP_URL
                      #   value: llm-d-redis-master.llm-d.svc.cluster.local:8100
                      - name: VLLM_NIXL_SIDE_CHANNEL_PORT
                        value: "6555"
                      - name: VLLM_NIXL_SIDE_CHANNEL_HOST
                        valueFrom:
                          fieldRef:
                            fieldPath: status.podIP

                    securityContext:
                      capabilities:
                        add: [ "IPC_LOCK" ]
                    # startupProbe:
                    #   httpGet:
                    #     path: /health
                    #     port: {{ "app_port" | getPort }}
                    #   failureThreshold: 60
                    #   initialDelaySeconds: 15
                    #   periodSeconds: 30
                    #   timeoutSeconds: 5
                    # livenessProbe:
                    #   tcpSocket:
                    #     port: {{ "app_port" | getPort }}
                    #   failureThreshold: 3
                    #   periodSeconds: 5
                    # readinessProbe:
                    #   httpGet:
                    #     path: /health
                    #     port: {{ "app_port" | getPort }}
                    #   failureThreshold: 3
                    #   periodSeconds: 5
                    resources:
                      limits:
                        nvidia.com/gpu: 8
                        ephemeral-storage: 256Gi
                        rdma/ib: 1
                      requests:
                        cpu: 8
                        memory: 750Gi
                        ephemeral-storage: 256Gi
                        nvidia.com/gpu: 8
                        rdma/ib: 1
                    volumeMounts:
                      - mountPath: /dev/shm
                        name: dshm
                      - name: init-scripts-volume
                        mountPath: /init-scripts
                  volumes:
                    # Volume for the init script from ConfigMap
                    - name: init-scripts-volume
                      configMap:
                        name: vllm-init-scripts-config
                        defaultMode: 0755 # Set execute permissions for the script
                    # Needed for NCCL to function
                    - name: dshm
                      emptyDir:
                        medium: Memory
                        sizeLimit: 16Gi

  prefillLeaderWorkerSet: |
    apiVersion: leaderworkerset.x-k8s.io/v1
    kind: LeaderWorkerSet
    metadata:
        name: vllm-prefill
    spec:
        startupPolicy: LeaderCreated
        leaderWorkerTemplate:
            size: {{ div .PrefillTensorParallelism 8 }}
            restartPolicy: RecreateGroupOnPodRestart

            leaderTemplate:

                spec:

                  containers:
                  - name: vllm-worker
                    # image: docker.io/alexeldeib/vllm:25e7af5@sha256:f710d0ce52daba6e35bb2078f2ca1318d5c0c3b4e3b62c6677d063c12d568bee
                    # image: docker.io/alexeldeib/vllm:25e7af5-docker-pypi@sha256:ddf630df17d3cb40ebea622f418eafd4eef048761268b2fdfd3b8b873a3486e3
                    # image: docker.io/alexeldeib/vllm:25e7af5-depot-src@sha256:2ff1117900b55a6c62364b84489209ca2ade59f137a045d383e9d3f7f3b4b287
                    image: docker.io/alexeldeib/llm-d:0.0.8-25e7af5
                    # image: "docker.io/alexeldeib/vllm:25e7af5@sha256:333996559c076c87e7e300f99807bf37585aa1e51487075fac35714853ffe834"
                    imagePullPolicy: Always
                    workingDir: /app
                    stdin: true
                    tty: true
                    command: ["/bin/sh","-c"]
                    args:
                      - |
                        # Squash a warning.
                        rm /etc/libibverbs.d/vmw_pvrdma.driver
                        #################
                        # Install vLLM
                        #################
                        ### /init-scripts/init-vllm.sh
                        #################
                        # RUN vLLM
                        #################
                        START_RANK=$(( ${LWS_WORKER_INDEX:-0} * DP_SIZE_LOCAL ))
                        if [ "${LWS_WORKER_INDEX:-0}" -eq 0 ]; then
                          #################
                          # Leader-only launch
                          #################
                          ray start --head
                          exec vllm serve \
                            {{ .HFModelName }} \
                            --port {{ "app_port" | getPort }} \
                            --disable-log-requests \
                            --enable-expert-parallel \
                            --tensor-parallel-size $TP_SIZE \
                            --data-parallel-size $DP_SIZE \
                            --data-parallel-size-local $DP_SIZE_LOCAL \
                            --data-parallel-address $(LWS_LEADER_ADDRESS).svc.cluster.local \
                            --data-parallel-rpc-port 5555 \
                            --trust-remote-code \
                            --enforce-eager
                        else
                          #################
                          # Worker-only launch
                          #################
                          ray start --block --address=$(LWS_LEADER_ADDRESS).svc.cluster.local:6379
                          # exec vllm serve \
                          #   {{ .HFModelName }} \
                          #   --port {{ "app_port" | getPort }} \
                          #   --disable-log-requests \
                          #   --enable-expert-parallel \
                          #   --tensor-parallel-size $TP_SIZE \
                          #   --data-parallel-size $DP_SIZE \
                          #   --data-parallel-size-local $DP_SIZE_LOCAL \
                          #   --data-parallel-address $(LWS_LEADER_ADDRESS).svc.cluster.local \
                          #   --data-parallel-rpc-port 5555 \
                          #   --trust-remote-code \
                          #   --kv-transfer-config \
                          #     '{"kv_connector":"NixlConnector","kv_role":"kv_both"}' \
                          #   --enforce-eager
                        fi
                    env:
                      - name: RAY_CGRAPH_get_timeout
                        value: "600"
                      - name: DP_SIZE
                        value: "{{ .PrefillDataParallelism }}"
                      - name: TP_SIZE
                        value: "{{ .PrefillTensorParallelism }}"
                      - name: DP_SIZE_LOCAL
                        value: "1"
                      - name: VLLM_REPO_URL
                        value: "https://github.com/vllm-project/vllm.git"
                      - name: VLLM_BRANCH
                        value: "main"
                      - name: VLLM_ALL2ALL_BACKEND
    #                    value: "naive"
                        # value: "pplx"
                        value: "deepep_high_throughput"
    #                    value: "deepep_low_latency"
    #
                        # Needed for GDRCOPY to be used.
                        # See: https://github.com/NVIDIA/nvidia-container-toolkit/releases/tag/v1.15.0
                      - name: NVIDIA_GDRCOPY
                        value: "enabled"
    #                  - name: NVIDIA_NVSWITCH
    #                    value: "enabled"
    #                  - name: NVIDIA_GDS
    #                    value: "enabled"

                      # NVIDIA_MOFED is likely needed for using IBGDA but causes crashes
    #                  - name: NVIDIA_MOFED
    #                    value: "enabled"
    #
                      - name: NCCL_DEBUG
                        value: "INFO"
                      - name: NVSHMEM_DEBUG
                        value: "TRACE"
                      - name: NVSHMEM_DEBUG_SUBSYS
                        value: "TRANSPORT,INIT,MEM,COLL,BOOTSTRAP"
                      - name: NVSHMEM_REMOTE_TRANSPORT
                        value: "ibrc"
                      - name: NVSHMEM_IB_ENABLE_IBGDA
                        value: "true"
                      - name: NVSHMEM_ENABLE_NIC_PE_MAPPING
                        value: "true"
                      - name: NVSHMEM_HCA_LIST
                        value: "ibp0:1,ibp1:1,ibp2:1,ibp3:1,ibp4:1,ibp5:1,ibp6:1,ibp7:1"
                      - name: NVSHMEM_BOOTSTRAP_UID_SOCK_IFNAME
                        value: "eth0"
                      - name: GLOO_SOCKET_IFNAME
                        value: "eth0"
                      - name: NCCL_SOCKET_IFNAME
                        value: "eth0"
                      - name: NCCL_IB_HCA
                        value: "ibp"
                      - name: UCX_NET_DEVICES
                        value: "ibp0:1,ibp1:1,ibp2:1,ibp3:1,ibp4:1,ibp5:1,ibp6:1,ibp7:1"
                      - name: VLLM_LOGGING_LEVEL
                        value: "DEBUG"
                      - name: HF_TOKEN
                        valueFrom:
                          secretKeyRef:
                            name: llm-d-hf-token
                            key: HF_TOKEN
                            optional: true
                      - name: GH_TOKEN_FROM_SECRET
                        valueFrom:
                          secretKeyRef:
                            name: gh-token-secret
                            key: GH_TOKEN
                            optional: true
                      - name: LMCACHE_DISTRIBUTED_URL
                        valueFrom:
                          fieldRef:
                            fieldPath: status.podIP
                      - name: UCX_TLS
                        value: "^cuda_ipc"
                      # - name: LMCACHE_ENABLE_DEBUG
                      #   value: "True"
                      # - name: LMCACHE_LOCAL_CPU
                      #   value: "True"
                      # - name: LMCACHE_MAX_LOCAL_CPU_SIZE
                      #   value: "5"
                      # - name: LMCACHE_MAX_LOCAL_DISK_SIZE
                      #   value: "10"
                      # - name: LMCACHE_CHUNK_SIZE
                      #   value: "256"
                      # - name: LMCACHE_LOOKUP_URL
                      #   value: llm-d-redis-master.llm-d.svc.cluster.local:8100
                      - name: VLLM_NIXL_SIDE_CHANNEL_PORT
                        value: "6555"
                      - name: VLLM_NIXL_SIDE_CHANNEL_HOST
                        valueFrom:
                          fieldRef:
                            fieldPath: status.podIP

                    securityContext:
                      capabilities:
                        add: [ "IPC_LOCK" ]
                    # startupProbe:
                    #   httpGet:
                    #     path: /health
                    #     port: {{ "app_port" | getPort }}
                    #   failureThreshold: 60
                    #   initialDelaySeconds: 15
                    #   periodSeconds: 30
                    #   timeoutSeconds: 5
                    # livenessProbe:
                    #   tcpSocket:
                    #     port: {{ "app_port" | getPort }}
                    #   failureThreshold: 3
                    #   periodSeconds: 5
                    # readinessProbe:
                    #   httpGet:
                    #     path: /health
                    #     port: {{ "app_port" | getPort }}
                    #   failureThreshold: 3
                    #   periodSeconds: 5
                    resources:
                      limits:
                        nvidia.com/gpu: 8
                        ephemeral-storage: 256Gi
                        rdma/ib: 1
                      requests:
                        cpu: 8
                        memory: 750Gi
                        ephemeral-storage: 256Gi
                        nvidia.com/gpu: 8
                        rdma/ib: 1
                    volumeMounts:
                      - mountPath: /dev/shm
                        name: dshm
                      - name: init-scripts-volume
                        mountPath: /init-scripts
                  volumes:
                    # Volume for the init script from ConfigMap
                    - name: init-scripts-volume
                      configMap:
                        name: vllm-init-scripts-config
                        defaultMode: 0755 # Set execute permissions for the script
                    # Needed for NCCL to function
                    - name: dshm
                      emptyDir:
                        medium: Memory
                        sizeLimit: 16Gi

            workerTemplate:

                spec:

                  containers:
                  - name: vllm-worker
                    # image: docker.io/alexeldeib/vllm:25e7af5@sha256:f710d0ce52daba6e35bb2078f2ca1318d5c0c3b4e3b62c6677d063c12d568bee
                    # image: docker.io/alexeldeib/vllm:25e7af5-docker-pypi@sha256:ddf630df17d3cb40ebea622f418eafd4eef048761268b2fdfd3b8b873a3486e3
                    # image: docker.io/alexeldeib/vllm:25e7af5-depot-src@sha256:2ff1117900b55a6c62364b84489209ca2ade59f137a045d383e9d3f7f3b4b287
                    image: docker.io/alexeldeib/llm-d:0.0.8-25e7af5
                    # image: "docker.io/alexeldeib/vllm:25e7af5@sha256:333996559c076c87e7e300f99807bf37585aa1e51487075fac35714853ffe834"
                    imagePullPolicy: Always
                    workingDir: /app
                    stdin: true
                    tty: true
                    command: ["/bin/sh","-c"]
                    args:
                      - |
                        # Squash a warning.
                        rm /etc/libibverbs.d/vmw_pvrdma.driver
                        #################
                        # Install vLLM
                        #################
                        ### /init-scripts/init-vllm.sh
                        #################
                        # RUN vLLM
                        #################
                        START_RANK=$(( ${LWS_WORKER_INDEX:-0} * DP_SIZE_LOCAL ))
                        if [ "${LWS_WORKER_INDEX:-0}" -eq 0 ]; then
                          #################
                          # Leader-only launch
                          #################
                          ray start --head --block &
                          pid=$!
                          exec vllm serve \
                            {{ .HFModelName }} \
                            --served-model-name {{ .HFModelName }} \
                            --port {{ "app_port" | getPort }} \
                            --disable-log-requests \
                            --enable-expert-parallel \
                            --tensor-parallel-size $TP_SIZE \
                            --data-parallel-size $DP_SIZE \
                            --data-parallel-size-local $DP_SIZE_LOCAL \
                            --data-parallel-address $(LWS_LEADER_ADDRESS) \
                            --data-parallel-rpc-port 5555 \
                            --trust-remote-code \
                            --enforce-eager
                        else
                          #################
                          # Worker-only launch
                          #################
                          ray start --block --address=$(LWS_LEADER_ADDRESS).svc.cluster.local:6379
                          # exec vllm serve \
                          #   {{ .HFModelName }} \
                          #   --served-model-name {{ .HFModelName }} \
                          #   --port {{ "app_port" | getPort }} \
                          #   --disable-log-requests \
                          #   --enable-expert-parallel \
                          #   --tensor-parallel-size $TP_SIZE \
                          #   --data-parallel-size $DP_SIZE \
                          #   --data-parallel-size-local $DP_SIZE_LOCAL \
                          #   --data-parallel-address $(LWS_LEADER_ADDRESS) \
                          #   --data-parallel-rpc-port 5555 \
                          #   --trust-remote-code \
                          #   --kv-transfer-config \
                          #     '{"kv_connector":"NixlConnector","kv_role":"kv_both"}' \
                          #   --enforce-eager
                        fi
                    env:
                      - name: RAY_CGRAPH_get_timeout
                        value: "600"
                      - name: DP_SIZE
                        value: "{{ .PrefillDataParallelism }}"
                      - name: TP_SIZE
                        value: "{{ .PrefillTensorParallelism }}"
                      - name: DP_SIZE_LOCAL
                        value: "1"
                      - name: VLLM_REPO_URL
                        value: "https://github.com/vllm-project/vllm.git"
                      - name: VLLM_BRANCH
                        value: "main"
                      - name: VLLM_ALL2ALL_BACKEND
    #                    value: "naive"
                        # value: "pplx"
                        value: "deepep_high_throughput"
    #                    value: "deepep_low_latency"
    #
                        # Needed for GDRCOPY to be used.
                        # See: https://github.com/NVIDIA/nvidia-container-toolkit/releases/tag/v1.15.0
                      - name: NVIDIA_GDRCOPY
                        value: "enabled"
    #                  - name: NVIDIA_NVSWITCH
    #                    value: "enabled"
    #                  - name: NVIDIA_GDS
    #                    value: "enabled"

                      # NVIDIA_MOFED is likely needed for using IBGDA but causes crashes
    #                  - name: NVIDIA_MOFED
    #                    value: "enabled"
    #
                      - name: NCCL_DEBUG
                        value: "INFO"
                      - name: NVSHMEM_DEBUG
                        value: "TRACE"
                      - name: NVSHMEM_DEBUG_SUBSYS
                        value: "TRANSPORT,INIT,MEM,COLL,BOOTSTRAP"
                      - name: NVSHMEM_REMOTE_TRANSPORT
                        value: "ibrc"
                      - name: NVSHMEM_IB_ENABLE_IBGDA
                        value: "true"
                      - name: NVSHMEM_ENABLE_NIC_PE_MAPPING
                        value: "true"
                      - name: NVSHMEM_HCA_LIST
                        value: "ibp0:1,ibp1:1,ibp2:1,ibp3:1,ibp4:1,ibp5:1,ibp6:1,ibp7:1"
                      - name: NVSHMEM_BOOTSTRAP_UID_SOCK_IFNAME
                        value: "eth0"
                      - name: GLOO_SOCKET_IFNAME
                        value: "eth0"
                      - name: NCCL_SOCKET_IFNAME
                        value: "eth0"
                      - name: NCCL_IB_HCA
                        value: "ibp"
                      - name: UCX_NET_DEVICES
                        value: "ibp0:1,ibp1:1,ibp2:1,ibp3:1,ibp4:1,ibp5:1,ibp6:1,ibp7:1"
                      - name: VLLM_LOGGING_LEVEL
                        value: "DEBUG"
                      - name: HF_TOKEN
                        valueFrom:
                          secretKeyRef:
                            name: llm-d-hf-token
                            key: HF_TOKEN
                            optional: true
                      - name: GH_TOKEN_FROM_SECRET
                        valueFrom:
                          secretKeyRef:
                            name: gh-token-secret
                            key: GH_TOKEN
                            optional: true
                      - name: LMCACHE_DISTRIBUTED_URL
                        valueFrom:
                          fieldRef:
                            fieldPath: status.podIP
                      - name: UCX_TLS
                        value: "^cuda_ipc"
                      # - name: LMCACHE_ENABLE_DEBUG
                      #   value: "True"
                      # - name: LMCACHE_LOCAL_CPU
                      #   value: "True"
                      # - name: LMCACHE_MAX_LOCAL_CPU_SIZE
                      #   value: "5"
                      # - name: LMCACHE_MAX_LOCAL_DISK_SIZE
                      #   value: "10"
                      # - name: LMCACHE_CHUNK_SIZE
                      #   value: "256"
                      # - name: LMCACHE_LOOKUP_URL
                      #   value: llm-d-redis-master.llm-d.svc.cluster.local:8100
                      - name: VLLM_NIXL_SIDE_CHANNEL_PORT
                        value: "6555"
                      - name: VLLM_NIXL_SIDE_CHANNEL_HOST
                        valueFrom:
                          fieldRef:
                            fieldPath: status.podIP

                    securityContext:
                      capabilities:
                        add: [ "IPC_LOCK" ]
                    # startupProbe:
                    #   httpGet:
                    #     path: /health
                    #     port: {{ "app_port" | getPort }}
                    #   failureThreshold: 60
                    #   initialDelaySeconds: 15
                    #   periodSeconds: 30
                    #   timeoutSeconds: 5
                    # livenessProbe:
                    #   tcpSocket:
                    #     port: {{ "app_port" | getPort }}
                    #   failureThreshold: 3
                    #   periodSeconds: 5
                    # readinessProbe:
                    #   httpGet:
                    #     path: /health
                    #     port: {{ "app_port" | getPort }}
                    #   failureThreshold: 3
                    #   periodSeconds: 5
                    resources:
                      limits:
                        nvidia.com/gpu: 8
                        ephemeral-storage: 256Gi
                        rdma/ib: 1
                      requests:
                        cpu: 8
                        memory: 750Gi
                        ephemeral-storage: 256Gi
                        nvidia.com/gpu: 8
                        rdma/ib: 1
                    volumeMounts:
                      - mountPath: /dev/shm
                        name: dshm
                      - name: init-scripts-volume
                        mountPath: /init-scripts

                  volumes:
                    # Volume for the init script from ConfigMap
                    - name: init-scripts-volume
                      configMap:
                        name: vllm-init-scripts-config
                        defaultMode: 0755 # Set execute permissions for the script
                    # Needed for NCCL to function
                    - name: dshm
                      emptyDir:
                        medium: Memory
                        sizeLimit: 16Gi

  eppService: |
    apiVersion: v1
    kind: Service
    spec:
      ports:
        # Needs to match the port of the eppDeployment
        - port: 9002    
          protocol: TCP
          targetPort: 9002 
          appProtocol: http2
      type: ClusterIP
  
  eppDeployment: |
    apiVersion: apps/v1
    kind: Deployment
    spec:
      template:
        spec:
          containers:
            - name: "epp"
              args:
                - -poolName
                - {{ .InferencePoolName }}
                - -poolNamespace
                - llm-d
                - -v
                - "5"
                - --zap-encoder
                - json
                - -grpcPort
                - "9002"
                - -grpcHealthPort
                - "9003"
              env:
                - name: ENABLE_KVCACHE_AWARE_SCORER
                  value: true
                - name: ENABLE_LOAD_AWARE_SCORER
                  value: true
                - name: ENABLE_PREFIX_AWARE_SCORER
                  value: true
                - name: ENABLE_SESSION_AWARE_SCORER
                  value: true
                - name: KVCACHE_AWARE_SCORER_WEIGHT
                  value: 1
                - name: KVCACHE_INDEXER_REDIS_ADDR
                  value: llm-d-redis-master.llm-d.svc.cluster.local:8100
                - name: LOAD_AWARE_SCORER_WEIGHT
                  value: 1
                - name: PD_ENABLED
                  value: true
                - name: PD_PROMPT_LEN_THRESHOLD
                  value: 10
                - name: PREFILL_ENABLE_KVCACHE_AWARE_SCORER
                  value: true
                - name: PREFILL_ENABLE_LOAD_AWARE_SCORER
                  value: true
                - name: PREFILL_ENABLE_PREFIX_AWARE_SCORER
                  value: true
                - name: PREFILL_ENABLE_SESSION_AWARE_SCORER
                  value: true
                - name: PREFILL_KVCACHE_AWARE_SCORER_WEIGHT
                  value: 1
                - name: PREFILL_KVCACHE_INDEXER_REDIS_ADDR
                  value: llm-d-redis-master.llm-d.svc.cluster.local:8100
                - name: PREFILL_LOAD_AWARE_SCORER_WEIGHT
                  value: 1
                - name: PREFILL_PREFIX_AWARE_SCORER_WEIGHT
                  value: 1
                - name: PREFILL_SESSION_AWARE_SCORER_WEIGHT
                  value: 1
                - name: PREFIX_AWARE_SCORER_WEIGHT
                  value: 2
                - name: SESSION_AWARE_SCORER_WEIGHT
                  value: 1
              image: ghcr.io/llm-d/llm-d-inference-scheduler:0.0.3
              livenessProbe:
                failureThreshold: 3
                grpc:
                  port: 9003
                  service: envoy.service.ext_proc.v3.ExternalProcessor
                initialDelaySeconds: 5
                periodSeconds: 10
              ports:
                - containerPort: 9002
                  protocol: TCP
                - containerPort: 9003
                  protocol: TCP
                - containerPort: 9090
                  name: metrics
                  protocol: TCP
              readinessProbe:
                failureThreshold: 3
                grpc:
                  port: 9003
                  service: envoy.service.ext_proc.v3.ExternalProcessor
                initialDelaySeconds: 5
                periodSeconds: 10
  
  inferencePool: |
    apiVersion: inference.networking.x-k8s.io/v1alpha2
    kind: InferencePool
    spec:
      targetPortNumber: {{ "app_port" | getPort }}

  inferenceModel: |
    apiVersion: inference.networking.x-k8s.io/v1alpha2
    kind: InferenceModel

  # httpRoute: |
  #   apiVersion: gateway.networking.k8s.io/v1
  #   kind: HTTPRoute
  #   spec:
  #     parentRefs:
  #     - group: gateway.networking.k8s.io
  #       kind: Gateway
  #       name: traefik-gateway
  #     rules:
  #     - backendRefs:
  #       - group: inference.networking.x-k8s.io
  #         kind: InferencePool
  #         name: {{ .InferencePoolName }}
  #         port: {{ "app_port" | getPort }}