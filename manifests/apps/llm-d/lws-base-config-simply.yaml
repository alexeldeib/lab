# Requirements:
# Any consuming ModelService should define ports labeled:
#    - app_port - the external port number for the prefill and decode pods
#    - internal_port - the port number used by the sidecar to communicate with a vllm container
apiVersion: v1
kind: ConfigMap
metadata:
  name: lws-baseconfig-simple
  namespace: llm-d
data:
  decodeLeaderWorkerSet: |
    apiVersion: leaderworkerset.x-k8s.io/v1
    kind: LeaderWorkerSet
    metadata:
        name: vllm-decode
    spec:
        startupPolicy: LeaderCreated
        leaderWorkerTemplate:
            size: {{ .DecodeDataParallelism }}
            restartPolicy: RecreateGroupOnPodRestart

            leaderTemplate:
                spec:
                  initContainers:
                  - name: routing-proxy
                    image: ghcr.io/llm-d/llm-d-routing-sidecar:0.0.6
                    securityContext:
                      securityContext:
                        allowPrivilegeEscalation: false
                        capabilities:
                          drop:
                          - MKNOD
                    args:
                      - "--port={{ "app_port" | getPort }}"
                      - "--vllm-port={{ "internal_port" | getPort }}"
                      - "--connector=nixlv2"
                    ports:
                      - containerPort: {{ "app_port" | getPort }}
                        protocol: TCP
                    restartPolicy: Always
                    imagePullPolicy: IfNotPresent
                    livenessProbe:
                      tcpSocket:
                        port: {{ "app_port" | getPort }}
                      failureThreshold: 3
                      periodSeconds: 5
                    readinessProbe:
                      tcpSocket:
                        port: {{ "app_port" | getPort }}
                      failureThreshold: 3
                      periodSeconds: 5
                  containers:
                  - name: vllm-worker
                    image: ghcr.io/llm-d/llm-d:0.0.8
                    imagePullPolicy: Always
                    workingDir: /app
                    stdin: true
                    tty: true
                    command: ["/bin/sh","-c"]
                    args:
                      - |
                        ray disable-usage-stats
                        mkdir -p /vllm-workspace/examples/online_serving
                        curl -fsSL -o /vllm-workspace/examples/online_serving/multi-node-serving.sh https://raw.githubusercontent.com/vllm-project/vllm/main/examples/online_serving/multi-node-serving.sh
                        bash /vllm-workspace/examples/online_serving/multi-node-serving.sh leader --ray_cluster_size=${LWS_GROUP_SIZE}
                        # python3 -m pip install 'huggingface_hub[cli,hf_transfer,hf_xet]'
                        # HF_HUB_ENABLE_HF_TRANSFER=1 huggingface-cli download {{ .ModelName }} --local-dir {{ .MountedModelPath }}
                        python3 -m pip install blobfile
                        python3 -m vllm.entrypoints.openai.api_server \
                          --port {{ "internal_port" | getPort }} \
                          --model {{ .MountedModelPath }} \
                          --trust-remote-code \
                          --data-parallel-size ${DP_SIZE} \
                          --tensor-parallel-size ${TP_SIZE} \
                          --kv-transfer-config \
                          '{"kv_connector":"MultiConnector","kv_role":"kv_both","kv_connector_extra_config":{"connectors":[{"kv_connector":"NixlConnector","kv_role":"kv_both"},{"kv_connector":"LMCacheConnectorV1","kv_role":"kv_both"}]}}'
                    env:
                      - name: RAY_CGRAPH_get_timeout
                        value: "600"
                      - name: DP_SIZE
                        value: "{{ .DecodeDataParallelism }}"
                      - name: TP_SIZE
                        value: "{{ .DecodeTensorParallelism }}"
                      - name: DP_SIZE_LOCAL
                        value: "1"
                      - name: VLLM_REPO_URL
                        value: "https://github.com/vllm-project/vllm.git"
                      - name: VLLM_BRANCH
                        value: "main"
                      - name: VLLM_ALL2ALL_BACKEND
                        # value: "naive"
                        # value: "pplx"
                        value: "deepep_high_throughput"
                        # value: "deepep_low_latency"

                        # Needed for GDRCOPY to be used.
                        # See: https://github.com/NVIDIA/nvidia-container-toolkit/releases/tag/v1.15.0
                      - name: NVIDIA_GDRCOPY
                        value: "enabled"
                    #  - name: NVIDIA_NVSWITCH
                    #    value: "enabled"
                    #  - name: NVIDIA_GDS
                    #    value: "enabled"

                      # NVIDIA_MOFED is likely needed for using IBGDA but causes crashes
                    #  - name: NVIDIA_MOFED
                    #    value: "enabled"
    
                      - name: NCCL_DEBUG
                        value: "INFO"
                      - name: NCCL_NVLS_ENABLE
                        value: "0"
                      - name: NVSHMEM_DEBUG
                        value: "TRACE"
                      - name: NVSHMEM_DEBUG_SUBSYS
                        value: "TRANSPORT,INIT,MEM,COLL,BOOTSTRAP"
                      - name: NVSHMEM_REMOTE_TRANSPORT
                        value: "ibrc"
                      - name: NVSHMEM_IB_ENABLE_IBGDA
                        value: "true"
                      - name: NVSHMEM_ENABLE_NIC_PE_MAPPING
                        value: "true"
                      - name: NVSHMEM_HCA_LIST
                        value: "ibp0:1,ibp1:1,ibp2:1,ibp3:1,ibp4:1,ibp5:1,ibp6:1,ibp7:1"
                      - name: NVSHMEM_BOOTSTRAP_UID_SOCK_IFNAME
                        value: "eth0"
                      - name: GLOO_SOCKET_IFNAME
                        value: "eth0"
                      - name: NCCL_SOCKET_IFNAME
                        value: "eth0"
                      - name: NCCL_IB_HCA
                        value: "ibp"
                      - name: UCX_NET_DEVICES
                        value: "ibp0:1,ibp1:1,ibp2:1,ibp3:1,ibp4:1,ibp5:1,ibp6:1,ibp7:1"
                      - name: VLLM_LOGGING_LEVEL
                        value: "DEBUG"
                      - name: HF_TOKEN
                        valueFrom:
                          secretKeyRef:
                            name: llm-d-hf-token
                            key: HF_TOKEN
                            optional: true
                      - name: GH_TOKEN_FROM_SECRET
                        valueFrom:
                          secretKeyRef:
                            name: gh-token-secret
                            key: GH_TOKEN
                            optional: true
                      - name: VLLM_NIXL_SIDE_CHANNEL_PORT
                        value: "5557"
                      - name: VLLM_NIXL_SIDE_CHANNEL_HOST
                        valueFrom:
                          fieldRef:
                            fieldPath: status.podIP
                      - name: LMCACHE_DISTRIBUTED_URL
                        valueFrom:
                          fieldRef:
                            fieldPath: status.podIP
                      - name: UCX_TLS
                        value: "^cuda_ipc"
                      - name: LMCACHE_ENABLE_DEBUG
                        value: "True"
                      - name: LMCACHE_LOCAL_CPU
                        value: "True"
                      - name: LMCACHE_MAX_LOCAL_CPU_SIZE
                        value: "5"
                      - name: LMCACHE_MAX_LOCAL_DISK_SIZE
                        value: "10"
                      - name: LMCACHE_CHUNK_SIZE
                        value: "256"
                      - name: LMCACHE_LOOKUP_URL
                        value: llm-d-redis-master.llm-d.svc.cluster.local:8100
                    securityContext:
                      capabilities:
                        add: [ "IPC_LOCK" ]
                    # startupProbe:
                    #   httpGet:
                    #     path: /health
                    #     port: {{ "internal_port" | getPort }}
                    #   failureThreshold: 360
                    #   initialDelaySeconds: 15
                    #   periodSeconds: 30
                    #   timeoutSeconds: 5
                    # livenessProbe:
                    #   tcpSocket:
                    #     port: {{ "internal_port" | getPort }}
                    #   failureThreshold: 3
                    #   periodSeconds: 5
                    # readinessProbe:
                    #   httpGet:
                    #     path: /health
                    #     port: {{ "internal_port" | getPort }}
                    #   failureThreshold: 3
                    #   periodSeconds: 5
                    resources:
                      limits:
                        nvidia.com/gpu: {{ .DecodeTensorParallelism }}
                        ephemeral-storage: 256Gi
                        rdma/ib: 1
                      requests:
                        cpu: 8
                        memory: 750Gi
                        ephemeral-storage: 256Gi
                        nvidia.com/gpu: {{ .DecodeTensorParallelism }}
                        rdma/ib: 1
                    volumeMounts:
                      - mountPath: /dev/shm
                        name: dshm
                      # - mountPath: /weights
                      #   name: weights
                      {{- if .HFModelName }}
                      - name: model-cache
                        mountPath: /models
                      {{- end }}
                  volumes:
                    {{- if .HFModelName }}
                    - name: model-cache
                      emptyDir: {}
                    {{- else }}
                    - name: model-storage
                      persistentVolumeClaim:
                        claimName: model-storage
                    {{- end }}
                    # Needed for NCCL to function
                    - name: dshm
                      emptyDir:
                        medium: Memory
                        sizeLimit: 16Gi

            workerTemplate:    
                spec:
                  containers:
                  - name: vllm-worker
                    image: ghcr.io/llm-d/llm-d:0.0.8
                    imagePullPolicy: Always
                    workingDir: /app
                    stdin: true
                    tty: true
                    command: ["/bin/sh","-c"]
                    args:
                      - |
                        mkdir -p /vllm-workspace/examples/online_serving
                        curl -fsSL -o /vllm-workspace/examples/online_serving/multi-node-serving.sh https://raw.githubusercontent.com/vllm-project/vllm/main/examples/online_serving/multi-node-serving.sh
                        bash /vllm-workspace/examples/online_serving/multi-node-serving.sh worker --ray_address=${LWS_LEADER_ADDRESS}
                    env:
                      - name: RAY_CGRAPH_get_timeout
                        value: "600"
                      - name: DP_SIZE
                        value: "{{ .DecodeDataParallelism }}"
                      - name: TP_SIZE
                        value: "{{ .DecodeTensorParallelism }}"
                      - name: DP_SIZE_LOCAL
                        value: "1"
                      - name: VLLM_REPO_URL
                        value: "https://github.com/vllm-project/vllm.git"
                      - name: VLLM_BRANCH
                        value: "main"
                      - name: VLLM_ALL2ALL_BACKEND
                        # value: "naive"
                        # value: "pplx"
                        value: "deepep_high_throughput"
                        # value: "deepep_low_latency"

                        # Needed for GDRCOPY to be used.
                        # See: https://github.com/NVIDIA/nvidia-container-toolkit/releases/tag/v1.15.0
                      - name: NVIDIA_GDRCOPY
                        value: "enabled"
                    #  - name: NVIDIA_NVSWITCH
                    #    value: "enabled"
                    #  - name: NVIDIA_GDS
                    #    value: "enabled"

                      # NVIDIA_MOFED is likely needed for using IBGDA but causes crashes
                    #  - name: NVIDIA_MOFED
                    #    value: "enabled"
    
                      - name: NCCL_DEBUG
                        value: "INFO"
                      - name: NCCL_NVLS_ENABLE
                        value: "0"
                      - name: NVSHMEM_DEBUG
                        value: "TRACE"
                      - name: NVSHMEM_DEBUG_SUBSYS
                        value: "TRANSPORT,INIT,MEM,COLL,BOOTSTRAP"
                      - name: NVSHMEM_REMOTE_TRANSPORT
                        value: "ibrc"
                      - name: NVSHMEM_IB_ENABLE_IBGDA
                        value: "true"
                      - name: NVSHMEM_ENABLE_NIC_PE_MAPPING
                        value: "true"
                      - name: NVSHMEM_HCA_LIST
                        value: "ibp0:1,ibp1:1,ibp2:1,ibp3:1,ibp4:1,ibp5:1,ibp6:1,ibp7:1"
                      - name: NVSHMEM_BOOTSTRAP_UID_SOCK_IFNAME
                        value: "eth0"
                      - name: GLOO_SOCKET_IFNAME
                        value: "eth0"
                      - name: NCCL_SOCKET_IFNAME
                        value: "eth0"
                      - name: NCCL_IB_HCA
                        value: "ibp"
                      - name: UCX_NET_DEVICES
                        value: "ibp0:1,ibp1:1,ibp2:1,ibp3:1,ibp4:1,ibp5:1,ibp6:1,ibp7:1"
                      - name: VLLM_LOGGING_LEVEL
                        value: "DEBUG"
                      - name: HF_TOKEN
                        valueFrom:
                          secretKeyRef:
                            name: llm-d-hf-token
                            key: HF_TOKEN
                            optional: true
                      - name: GH_TOKEN_FROM_SECRET
                        valueFrom:
                          secretKeyRef:
                            name: gh-token-secret
                            key: GH_TOKEN
                            optional: true
                      - name: VLLM_NIXL_SIDE_CHANNEL_PORT
                        value: "5557"
                      - name: VLLM_NIXL_SIDE_CHANNEL_HOST
                        valueFrom:
                          fieldRef:
                            fieldPath: status.podIP
                      - name: LMCACHE_DISTRIBUTED_URL
                        valueFrom:
                          fieldRef:
                            fieldPath: status.podIP
                      - name: UCX_TLS
                        value: "^cuda_ipc"
                      - name: LMCACHE_ENABLE_DEBUG
                        value: "True"
                      - name: LMCACHE_LOCAL_CPU
                        value: "True"
                      - name: LMCACHE_MAX_LOCAL_CPU_SIZE
                        value: "5"
                      - name: LMCACHE_MAX_LOCAL_DISK_SIZE
                        value: "10"
                      - name: LMCACHE_CHUNK_SIZE
                        value: "256"
                      - name: LMCACHE_LOOKUP_URL
                        value: llm-d-redis-master.llm-d.svc.cluster.local:8100
                    securityContext:
                      capabilities:
                        add: [ "IPC_LOCK" ]
                    # startupProbe:
                    #   httpGet:
                    #     path: /health
                    #     port: {{ "internal_port" | getPort }}
                    #   failureThreshold: 60
                    #   initialDelaySeconds: 15
                    #   periodSeconds: 30
                    #   timeoutSeconds: 5
                    # livenessProbe:
                    #   tcpSocket:
                    #     port: {{ "internal_port" | getPort }}
                    #   failureThreshold: 3
                    #   periodSeconds: 5
                    # readinessProbe:
                    #   httpGet:
                    #     path: /health
                    #     port: {{ "internal_port" | getPort }}
                    #   failureThreshold: 3
                    #   periodSeconds: 5
                    resources:
                      limits:
                        nvidia.com/gpu: {{ .DecodeTensorParallelism }}
                        ephemeral-storage: 256Gi
                        rdma/ib: 1
                      requests:
                        cpu: 8
                        memory: 750Gi
                        ephemeral-storage: 256Gi
                        nvidia.com/gpu: {{ .DecodeTensorParallelism }}
                        rdma/ib: 1
                    volumeMounts:
                      - mountPath: /dev/shm
                        name: dshm
                      {{- if .HFModelName }}
                      - name: model-cache
                        mountPath: /models
                      {{- end }}
                  volumes:
                    {{- if .HFModelName }}
                    - name: model-cache
                      emptyDir: {}
                    {{- else }}
                    - name: model-storage
                      persistentVolumeClaim:
                        claimName: model-storage
                    {{- end }}
                    # Needed for NCCL to function
                    - name: dshm
                      emptyDir:
                        medium: Memory
                        sizeLimit: 16Gi

  prefillLeaderWorkerSet: |
    apiVersion: leaderworkerset.x-k8s.io/v1
    kind: LeaderWorkerSet
    metadata:
        name: vllm-prefill
    spec:
        startupPolicy: LeaderCreated
        leaderWorkerTemplate:
            size: {{ .DecodeDataParallelism }}
            restartPolicy: RecreateGroupOnPodRestart

            leaderTemplate:
                spec:
                  initContainers:
                  - name: routing-proxy
                    image: ghcr.io/llm-d/llm-d-routing-sidecar:0.0.6
                    securityContext:
                      securityContext:
                        allowPrivilegeEscalation: false
                        capabilities:
                          drop:
                          - MKNOD
                    args:
                      - "--port={{ "app_port" | getPort }}"
                      - "--vllm-port={{ "internal_port" | getPort }}"
                      - "--connector=nixlv2"
                    ports:
                      - containerPort: {{ "app_port" | getPort }}
                        protocol: TCP
                    restartPolicy: Always
                    imagePullPolicy: IfNotPresent
                    livenessProbe:
                      tcpSocket:
                        port: {{ "app_port" | getPort }}
                      failureThreshold: 3
                      periodSeconds: 5
                    readinessProbe:
                      tcpSocket:
                        port: {{ "app_port" | getPort }}
                      failureThreshold: 3
                      periodSeconds: 5
                  containers:
                  - name: vllm-worker
                    image: ghcr.io/llm-d/llm-d:0.0.8
                    imagePullPolicy: Always
                    workingDir: /app
                    stdin: true
                    tty: true
                    command: ["/bin/sh","-c"]
                    args:
                      - |
                        ray disable-usage-stats
                        mkdir -p /vllm-workspace/examples/online_serving
                        curl -fsSL -o /vllm-workspace/examples/online_serving/multi-node-serving.sh https://raw.githubusercontent.com/vllm-project/vllm/main/examples/online_serving/multi-node-serving.sh
                        bash /vllm-workspace/examples/online_serving/multi-node-serving.sh leader --ray_cluster_size=${LWS_GROUP_SIZE}
                        #python3 -m pip install 'huggingface_hub[cli,hf_transfer,hf_xet]'
                        #HF_HUB_ENABLE_HF_TRANSFER=1 huggingface-cli download {{ .ModelName }} --local-dir {{ .MountedModelPath }}
                        python3 -m pip install blobfile
                        python3 -m vllm.entrypoints.openai.api_server \
                          --port {{ "internal_port" | getPort }} \
                          --model {{ .MountedModelPath }} \
                          --trust-remote-code \
                          --data-parallel-size ${DP_SIZE} \
                          --tensor-parallel-size ${TP_SIZE} \
                          --kv-transfer-config \
                          '{"kv_connector":"MultiConnector","kv_role":"kv_both","kv_connector_extra_config":{"connectors":[{"kv_connector":"NixlConnector","kv_role":"kv_both"},{"kv_connector":"LMCacheConnectorV1","kv_role":"kv_both"}]}}'
                    env:
                      - name: RAY_CGRAPH_get_timeout
                        value: "600"
                      - name: DP_SIZE
                        value: "{{ .PrefillDataParallelism }}"
                      - name: TP_SIZE
                        value: "{{ .PrefillTensorParallelism }}"
                      - name: DP_SIZE_LOCAL
                        value: "1"
                      - name: VLLM_REPO_URL
                        value: "https://github.com/vllm-project/vllm.git"
                      - name: VLLM_BRANCH
                        value: "main"
                      - name: VLLM_ALL2ALL_BACKEND
    #                    value: "naive"
                        # value: "pplx"
                        value: "deepep_high_throughput"
    #                    value: "deepep_low_latency"
    #
                        # Needed for GDRCOPY to be used.
                        # See: https://github.com/NVIDIA/nvidia-container-toolkit/releases/tag/v1.15.0
                      - name: NVIDIA_GDRCOPY
                        value: "enabled"
    #                  - name: NVIDIA_NVSWITCH
    #                    value: "enabled"
    #                  - name: NVIDIA_GDS
    #                    value: "enabled"

                      # NVIDIA_MOFED is likely needed for using IBGDA but causes crashes
    #                  - name: NVIDIA_MOFED
    #                    value: "enabled"
    #
                      - name: NCCL_DEBUG
                        value: "INFO"
                      - name: NCCL_NVLS_ENABLE
                        value: "0"
                      - name: NVSHMEM_DEBUG
                        value: "TRACE"
                      - name: NVSHMEM_DEBUG_SUBSYS
                        value: "TRANSPORT,INIT,MEM,COLL,BOOTSTRAP"
                      - name: NVSHMEM_REMOTE_TRANSPORT
                        value: "ibrc"
                      - name: NVSHMEM_IB_ENABLE_IBGDA
                        value: "true"
                      - name: NVSHMEM_ENABLE_NIC_PE_MAPPING
                        value: "true"
                      - name: NVSHMEM_HCA_LIST
                        value: "ibp0:1,ibp1:1,ibp2:1,ibp3:1,ibp4:1,ibp5:1,ibp6:1,ibp7:1"
                      - name: NVSHMEM_BOOTSTRAP_UID_SOCK_IFNAME
                        value: "eth0"
                      - name: GLOO_SOCKET_IFNAME
                        value: "eth0"
                      - name: NCCL_SOCKET_IFNAME
                        value: "eth0"
                      - name: NCCL_IB_HCA
                        value: "ibp"
                      - name: UCX_NET_DEVICES
                        value: "ibp0:1,ibp1:1,ibp2:1,ibp3:1,ibp4:1,ibp5:1,ibp6:1,ibp7:1"
                      - name: VLLM_LOGGING_LEVEL
                        value: "DEBUG"
                      - name: HF_TOKEN
                        valueFrom:
                          secretKeyRef:
                            name: llm-d-hf-token
                            key: HF_TOKEN
                            optional: true
                      - name: GH_TOKEN_FROM_SECRET
                        valueFrom:
                          secretKeyRef:
                            name: gh-token-secret
                            key: GH_TOKEN
                            optional: true
                      - name: VLLM_NIXL_SIDE_CHANNEL_PORT
                        value: "5557"
                      - name: VLLM_NIXL_SIDE_CHANNEL_HOST
                        valueFrom:
                          fieldRef:
                            fieldPath: status.podIP
                      - name: LMCACHE_DISTRIBUTED_URL
                        valueFrom:
                          fieldRef:
                            fieldPath: status.podIP
                      - name: UCX_TLS
                        value: "^cuda_ipc"
                      - name: LMCACHE_ENABLE_DEBUG
                        value: "True"
                      - name: LMCACHE_LOCAL_CPU
                        value: "True"
                      - name: LMCACHE_MAX_LOCAL_CPU_SIZE
                        value: "5"
                      - name: LMCACHE_MAX_LOCAL_DISK_SIZE
                        value: "10"
                      - name: LMCACHE_CHUNK_SIZE
                        value: "256"
                      - name: LMCACHE_LOOKUP_URL
                        value: llm-d-redis-master.llm-d.svc.cluster.local:8100
                    securityContext:
                      capabilities:
                        add: [ "IPC_LOCK" ]
                    # startupProbe:
                    #   httpGet:
                    #     path: /health
                    #     port: {{ "app_port" | getPort }}
                    #   failureThreshold: 360
                    #   initialDelaySeconds: 15
                    #   periodSeconds: 30
                    #   timeoutSeconds: 5
                    # livenessProbe:
                    #   tcpSocket:
                    #     port: {{ "app_port" | getPort }}
                    #   failureThreshold: 3
                    #   periodSeconds: 5
                    # readinessProbe:
                    #   httpGet:
                    #     path: /health
                    #     port: {{ "app_port" | getPort }}
                    #   failureThreshold: 3
                    #   periodSeconds: 5
                    resources:
                      limits:
                        nvidia.com/gpu: {{ .DecodeTensorParallelism }}
                        ephemeral-storage: 256Gi
                        rdma/ib: 1
                      requests:
                        cpu: 8
                        memory: 750Gi
                        ephemeral-storage: 256Gi
                        nvidia.com/gpu: {{ .DecodeTensorParallelism }}
                        rdma/ib: 1
                    volumeMounts:
                      - mountPath: /dev/shm
                        name: dshm
                      # - mountPath: /weights
                      #   name: weights
                      {{- if .HFModelName }}
                      - name: model-cache
                        mountPath: /models
                      {{- end }}
                  volumes:
                    {{- if .HFModelName }}
                    - name: model-cache
                      emptyDir: {}
                    {{- else }}
                    - name: model-storage
                      persistentVolumeClaim:
                        claimName: model-storage
                    {{- end }}
                    # Needed for NCCL to function
                    - name: dshm
                      emptyDir:
                        medium: Memory
                        sizeLimit: 16Gi

            workerTemplate:
                spec:
                  containers:
                  - name: vllm-worker
                    image: ghcr.io/llm-d/llm-d:0.0.8
                    imagePullPolicy: Always
                    workingDir: /app
                    stdin: true
                    tty: true
                    command: ["/bin/sh","-c"]
                    args:
                      - |
                        mkdir -p /vllm-workspace/examples/online_serving
                        curl -fsSL -o /vllm-workspace/examples/online_serving/multi-node-serving.sh https://raw.githubusercontent.com/vllm-project/vllm/main/examples/online_serving/multi-node-serving.sh
                        bash /vllm-workspace/examples/online_serving/multi-node-serving.sh worker --ray_address=${LWS_LEADER_ADDRESS}
                    env:
                      - name: RAY_CGRAPH_get_timeout
                        value: "600"
                      - name: DP_SIZE
                        value: "{{ .PrefillDataParallelism }}"
                      - name: TP_SIZE
                        value: "{{ .PrefillTensorParallelism }}"
                      - name: DP_SIZE_LOCAL
                        value: "1"
                      - name: VLLM_REPO_URL
                        value: "https://github.com/vllm-project/vllm.git"
                      - name: VLLM_BRANCH
                        value: "main"
                      - name: VLLM_ALL2ALL_BACKEND
    #                    value: "naive"
                        # value: "pplx"
                        value: "deepep_high_throughput"
    #                    value: "deepep_low_latency"
    #
                        # Needed for GDRCOPY to be used.
                        # See: https://github.com/NVIDIA/nvidia-container-toolkit/releases/tag/v1.15.0
                      - name: NVIDIA_GDRCOPY
                        value: "enabled"
    #                  - name: NVIDIA_NVSWITCH
    #                    value: "enabled"
    #                  - name: NVIDIA_GDS
    #                    value: "enabled"

                      # NVIDIA_MOFED is likely needed for using IBGDA but causes crashes
    #                  - name: NVIDIA_MOFED
    #                    value: "enabled"
    #
                      - name: NCCL_DEBUG
                        value: "INFO"
                      - name: NCCL_NVLS_ENABLE
                        value: "0"
                      - name: NVSHMEM_DEBUG
                        value: "TRACE"
                      - name: NVSHMEM_DEBUG_SUBSYS
                        value: "TRANSPORT,INIT,MEM,COLL,BOOTSTRAP"
                      - name: NVSHMEM_REMOTE_TRANSPORT
                        value: "ibrc"
                      - name: NVSHMEM_IB_ENABLE_IBGDA
                        value: "true"
                      - name: NVSHMEM_ENABLE_NIC_PE_MAPPING
                        value: "true"
                      - name: NVSHMEM_HCA_LIST
                        value: "ibp0:1,ibp1:1,ibp2:1,ibp3:1,ibp4:1,ibp5:1,ibp6:1,ibp7:1"
                      - name: NVSHMEM_BOOTSTRAP_UID_SOCK_IFNAME
                        value: "eth0"
                      - name: GLOO_SOCKET_IFNAME
                        value: "eth0"
                      - name: NCCL_SOCKET_IFNAME
                        value: "eth0"
                      - name: NCCL_IB_HCA
                        value: "ibp"
                      - name: UCX_NET_DEVICES
                        value: "ibp0:1,ibp1:1,ibp2:1,ibp3:1,ibp4:1,ibp5:1,ibp6:1,ibp7:1"
                      - name: VLLM_LOGGING_LEVEL
                        value: "DEBUG"
                      - name: HF_TOKEN
                        valueFrom:
                          secretKeyRef:
                            name: llm-d-hf-token
                            key: HF_TOKEN
                            optional: true
                      - name: GH_TOKEN_FROM_SECRET
                        valueFrom:
                          secretKeyRef:
                            name: gh-token-secret
                            key: GH_TOKEN
                            optional: true
                      - name: VLLM_NIXL_SIDE_CHANNEL_PORT
                        value: "5557"
                      - name: VLLM_NIXL_SIDE_CHANNEL_HOST
                        valueFrom:
                          fieldRef:
                            fieldPath: status.podIP
                      - name: LMCACHE_DISTRIBUTED_URL
                        valueFrom:
                          fieldRef:
                            fieldPath: status.podIP
                      - name: UCX_TLS
                        value: "^cuda_ipc"
                      - name: LMCACHE_ENABLE_DEBUG
                        value: "True"
                      - name: LMCACHE_LOCAL_CPU
                        value: "True"
                      - name: LMCACHE_MAX_LOCAL_CPU_SIZE
                        value: "5"
                      - name: LMCACHE_MAX_LOCAL_DISK_SIZE
                        value: "10"
                      - name: LMCACHE_CHUNK_SIZE
                        value: "256"
                      - name: LMCACHE_LOOKUP_URL
                        value: llm-d-redis-master.llm-d.svc.cluster.local:8100
                    securityContext:
                      capabilities:
                        add: [ "IPC_LOCK" ]
                    # startupProbe:
                    #   httpGet:
                    #     path: /health
                    #     port: {{ "app_port" | getPort }}
                    #   failureThreshold: 60
                    #   initialDelaySeconds: 15
                    #   periodSeconds: 30
                    #   timeoutSeconds: 5
                    # livenessProbe:
                    #   tcpSocket:
                    #     port: {{ "app_port" | getPort }}
                    #   failureThreshold: 3
                    #   periodSeconds: 5
                    # readinessProbe:
                    #   httpGet:
                    #     path: /health
                    #     port: {{ "app_port" | getPort }}
                    #   failureThreshold: 3
                    #   periodSeconds: 5
                    resources:
                      limits:
                        nvidia.com/gpu: {{ .DecodeTensorParallelism }}
                        ephemeral-storage: 256Gi
                        rdma/ib: 1
                      requests:
                        cpu: 8
                        memory: 750Gi
                        ephemeral-storage: 256Gi
                        nvidia.com/gpu: {{ .DecodeTensorParallelism }}
                        rdma/ib: 1
                    volumeMounts:
                      - mountPath: /dev/shm
                        name: dshm
                      # - mountPath: /weights
                      #   name: weights
                      {{- if .HFModelName }}
                      - name: model-cache
                        mountPath: /models
                      {{- end }}
                  volumes:
                    {{- if .HFModelName }}
                    - name: model-cache
                      emptyDir: {}
                    {{- else }}
                    - name: model-storage
                      persistentVolumeClaim:
                        claimName: model-storage
                    {{- end }}
                    # Needed for NCCL to function
                    - name: dshm
                      emptyDir:
                        medium: Memory
                        sizeLimit: 16Gi

  eppService: |
    apiVersion: v1
    kind: Service
    spec:
      ports:
        # Needs to match the port of the eppDeployment
        - port: 9002    
          protocol: TCP
          targetPort: 9002 
          appProtocol: http2
      type: ClusterIP
  
  eppDeployment: |
    apiVersion: apps/v1
    kind: Deployment
    spec:
      template:
        spec:
          containers:
            - name: "epp"
              args:
                - -poolName
                - {{ .InferencePoolName }}
                - -poolNamespace
                - llm-d
                - -v
                - "5"
                - --zap-encoder
                - json
                - -grpcPort
                - "9002"
                - -grpcHealthPort
                - "9003"
              env:
                - name: ENABLE_KVCACHE_AWARE_SCORER
                  value: true
                - name: ENABLE_LOAD_AWARE_SCORER
                  value: true
                - name: ENABLE_PREFIX_AWARE_SCORER
                  value: true
                - name: ENABLE_SESSION_AWARE_SCORER
                  value: true
                - name: KVCACHE_AWARE_SCORER_WEIGHT
                  value: 1
                - name: KVCACHE_INDEXER_REDIS_ADDR
                  value: llm-d-redis-master.llm-d.svc.cluster.local:8100
                - name: LOAD_AWARE_SCORER_WEIGHT
                  value: 1
                - name: PD_ENABLED
                  value: true
                - name: PD_PROMPT_LEN_THRESHOLD
                  value: 10
                - name: PREFILL_ENABLE_KVCACHE_AWARE_SCORER
                  value: true
                - name: PREFILL_ENABLE_LOAD_AWARE_SCORER
                  value: true
                - name: PREFILL_ENABLE_PREFIX_AWARE_SCORER
                  value: true
                - name: PREFILL_ENABLE_SESSION_AWARE_SCORER
                  value: true
                - name: PREFILL_KVCACHE_AWARE_SCORER_WEIGHT
                  value: 1
                - name: PREFILL_KVCACHE_INDEXER_REDIS_ADDR
                  value: llm-d-redis-master.llm-d.svc.cluster.local:8100
                - name: PREFILL_LOAD_AWARE_SCORER_WEIGHT
                  value: 1
                - name: PREFILL_PREFIX_AWARE_SCORER_WEIGHT
                  value: 1
                - name: PREFILL_SESSION_AWARE_SCORER_WEIGHT
                  value: 1
                - name: PREFIX_AWARE_SCORER_WEIGHT
                  value: 2
                - name: SESSION_AWARE_SCORER_WEIGHT
                  value: 1
              image: ghcr.io/llm-d/llm-d-inference-scheduler:0.0.3
              livenessProbe:
                failureThreshold: 3
                grpc:
                  port: 9003
                  service: envoy.service.ext_proc.v3.ExternalProcessor
                initialDelaySeconds: 5
                periodSeconds: 10
              ports:
                - containerPort: 9002
                  protocol: TCP
                - containerPort: 9003
                  protocol: TCP
                - containerPort: 9090
                  name: metrics
                  protocol: TCP
              readinessProbe:
                failureThreshold: 3
                grpc:
                  port: 9003
                  service: envoy.service.ext_proc.v3.ExternalProcessor
                initialDelaySeconds: 5
                periodSeconds: 10
  
  inferencePool: |
    apiVersion: inference.networking.x-k8s.io/v1alpha2
    kind: InferencePool
    spec:
      targetPortNumber: {{ "internal_port" | getPort }}

  inferenceModel: |
    apiVersion: inference.networking.x-k8s.io/v1alpha2
    kind: InferenceModel

  # httpRoute: |
  #   apiVersion: gateway.networking.k8s.io/v1
  #   kind: HTTPRoute
  #   spec:
  #     parentRefs:
  #     - group: gateway.networking.k8s.io
  #       kind: Gateway
  #       name: traefik-gateway
  #     rules:
  #     - backendRefs:
  #       - group: inference.networking.x-k8s.io
  #         kind: InferencePool
  #         name: {{ .InferencePoolName }}
  #         port: {{ "app_port" | getPort }}