apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: weights-pvc
  namespace: llm-d
spec:
  accessModes:
  - ReadWriteMany
  resources:
    requests:
      storage: 5Ti
  storageClassName: csi-s3
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: model-storage
  namespace: llm-d
spec:
  accessModes:
  - ReadWriteMany
  resources:
    requests:
      storage: 5Ti
  storageClassName: shared-vast
---
apiVersion: leaderworkerset.x-k8s.io/v1
kind: LeaderWorkerSet
metadata:
  name: vllm
  namespace: llm-d
spec:
  replicas: 1
  leaderWorkerTemplate:
    size: 2
    restartPolicy: RecreateGroupOnPodRestart
    leaderTemplate:
      metadata:
        labels:
          role: leader
      spec:
        containers:
          - name: vllm-leader
            # image: docker.io/vllm/vllm-openai:latest
            image: ghcr.io/llm-d/llm-d:0.0.8
            env:
              - name: VLLM_LOGGING_LEVEL
                value: "INFO"
              - name: RAY_CGRAPH_get_timeout
                value: "3600"
              - name: HUGGING_FACE_HUB_TOKEN
                valueFrom:
                  secretKeyRef:
                    name: llm-d-hf-token
                    key: HF_TOKEN
            command:
              - bash
              - -c
              - |
                mkdir -p /vllm-workspace/examples/online_serving
                curl -fsSL -o /vllm-workspace/examples/online_serving/multi-node-serving.sh https://raw.githubusercontent.com/vllm-project/vllm/main/examples/online_serving/multi-node-serving.sh
                bash /vllm-workspace/examples/online_serving/multi-node-serving.sh leader --ray_cluster_size=${LWS_GROUP_SIZE}
                pip install -U 'huggingface_hub[cli,hf_transfer]'
                MODEL='moonshotai/Kimi-K2-Instruct'
                # cd /weights/mosonshotai/Kimi-K2-Instruct
                HF_HUB_ENABLE_HF_TRANSFER=1 huggingface-cli download $MODEL --local-dir /weights/moonshotai/Kimi-K2-Instruct
                python3 -m pip install blobfile
                python3 -m vllm.entrypoints.openai.api_server --port 8080 --model /weights/${MODEL} --config-format auto --trust-remote-code --tensor-parallel-size 16
            resources:
              limits:
                nvidia.com/gpu: "8"
                ephemeral-storage: 800Gi
                rdma/ib: 1
              requests:
                ephemeral-storage: 800Gi
                memory: 1124Gi
                rdma/ib: 1
                cpu: 96
            ports:
              - containerPort: 8080
            readinessProbe:
              tcpSocket:
                port: 8080
              initialDelaySeconds: 15
              periodSeconds: 10
            volumeMounts:
              - mountPath: /dev/shm
                name: dshm
              - mountPath: /weights
                name: weights
        terminationGracePeriodSeconds: 15
        volumes:
        - name: weights
          persistentVolumeClaim:
            claimName: weights-vast-pvc
            readOnly: false
        - name: dshm
          emptyDir:
            medium: Memory
            sizeLimit: 16Gi
    workerTemplate:
      spec:
        containers:
          - name: vllm-worker
            # image: docker.io/vllm/vllm-openai:latest
            image: ghcr.io/llm-d/llm-d:0.0.8
            command:
              - sh
              - -c
              - |
                mkdir -p /vllm-workspace/examples/online_serving
                curl -fsSL -o /vllm-workspace/examples/online_serving/multi-node-serving.sh https://raw.githubusercontent.com/vllm-project/vllm/main/examples/online_serving/multi-node-serving.sh
                bash /vllm-workspace/examples/online_serving/multi-node-serving.sh worker --ray_address=${LWS_LEADER_ADDRESS}
            resources:
              limits:
                nvidia.com/gpu: "8"
                ephemeral-storage: 800Gi
                rdma/ib: 1
              requests:
                ephemeral-storage: 800Gi
                memory: 1124Gi
                rdma/ib: 1
                cpu: 125
            env:
              - name: VLLM_LOGGING_LEVEL
                value: "INFO"
              - name: RAY_CGRAPH_get_timeout
                value: "3600"
              - name: HUGGING_FACE_HUB_TOKEN
                valueFrom:
                  secretKeyRef:
                    name: llm-d-hf-token
                    key: HF_TOKEN
            volumeMounts:
              - mountPath: /dev/shm
                name: dshm
              - mountPath: /weights
                name: weights
        terminationGracePeriodSeconds: 15
        volumes:
        - name: weights
          persistentVolumeClaim:
            claimName: weights-vast-pvc
            readOnly: false
        - name: dshm
          emptyDir:
            medium: Memory
            sizeLimit: 16Gi
---
apiVersion: v1
kind: Service
metadata:
  name: vllm-leader
  namespace: llm-d
spec:
  ports:
    - name: http
      port: 8080
      protocol: TCP
      targetPort: 8080
  selector:
    leaderworkerset.sigs.k8s.io/name: vllm
    role: leader
  type: ClusterIP