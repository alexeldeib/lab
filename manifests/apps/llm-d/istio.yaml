apiVersion: gateway.networking.k8s.io/v1
kind: Gateway
metadata:
  annotations:
    networking.istio.io/service-type: ClusterIP
  labels:
    app.kubernetes.io/component: inference-gateway
    app.kubernetes.io/gateway: llm-d-inference-gateway
    app.kubernetes.io/instance: llm-d
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: llm-d
    app.kubernetes.io/version: "0.1"
    helm.sh/chart: llm-d-1.0.22
    istio.io/enable-inference-extproc: "true"
  name: llm-d-inference-gateway
  namespace: llm-d
spec:
  gatewayClassName: istio
  listeners:
  - name: default
    port: 80
    protocol: HTTP
---
apiVersion: gateway.networking.k8s.io/v1
kind: HTTPRoute
metadata:
  labels:
    app.kubernetes.io/component: sample-application
    app.kubernetes.io/instance: llm-d
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: llm-d
    app.kubernetes.io/version: "0.1"
    helm.sh/chart: llm-d-1.0.22
  name: meta-llama-llama-3-2-3b-instruct
  namespace: llm-d
spec:
  parentRefs:
  - name: llm-d-inference-gateway
  rules:
  - backendRefs:
    - group: inference.networking.x-k8s.io
      kind: InferencePool
      name: meta-llama-llama-3-2-3b-instruct-inference-pool
      port: 8000
    matches:
    - path:
        type: PathPrefix
        value: /
    timeouts:
      backendRequest: 0s
      request: 0s
---
apiVersion: networking.istio.io/v1
kind: DestinationRule
metadata:
  name: meta-llama-llama-3-2-3b-instruct-insecure-tls
  namespace: llm-d
spec:
  host: meta-llama-llama-3-2-3b-instruct-epp-service
  trafficPolicy:
    tls:
      insecureSkipVerify: true
      mode: SIMPLE
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  labels:
    app.kubernetes.io/component: sample-application
    app.kubernetes.io/gateway: llm-d-inference-gateway
    app.kubernetes.io/instance: llm-d
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: llm-d
    app.kubernetes.io/version: "0.1"
    helm.sh/chart: llm-d-1.0.22
  name: llm-d-inference-gateway
  namespace: llm-d
spec:
  rules:
  - host: llm-d-inference-gateway.localhost
    http:
      paths:
      - backend:
          service:
            name: llm-d-inference-gateway
            port:
              number: 80
        path: /
        pathType: Prefix